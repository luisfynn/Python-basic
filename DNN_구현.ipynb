{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "487d02f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f matrix : tensor([[21.]], grad_fn=<MmBackward0>)\n",
      "f = 21.0\n",
      "âˆ‡ f:tensor([[10.],\n",
      "        [16.]])\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch\n",
    "\n",
    "import torch\n",
    "\n",
    "#torch.tensor()ëŠ” í•¨ìˆ˜(function)\n",
    "#[[1.0], [2.0]]: 2Ã—1 ì—´ ë²¡í„° (Column vector)\n",
    "#requires_grad=True: ì´í›„ ë¯¸ë¶„(gradient)ì„ ê³„ì‚°í•  ìˆ˜ ìˆë„ë¡ ìë™ ë¯¸ë¶„ íŠ¸ë˜í‚¹ í™œì„±í™”\n",
    "x = torch.tensor([[1.0], [2.0]], requires_grad=True)\n",
    "\n",
    "#2Ã—2 ì‹¤ìˆ˜ í–‰ë ¬ì„ ìƒì„±. \n",
    "#ê¸°ë³¸ì ìœ¼ë¡œ requires_grad=Falseì´ë¯€ë¡œ Aì— ëŒ€í•´ ë¯¸ë¶„ì€ ê³„ì‚°ë˜ì§€ ì•ŠìŒ\n",
    "A = torch.tensor([[1.0, 2.0], [2.0, 3.0]])\n",
    "\n",
    "#torch.matmul()ì€ í•¨ìˆ˜(function)\n",
    "#torch.matmul(A, x) â†’ Ax: 2Ã—2 í–‰ë ¬ê³¼ 2Ã—1 ë²¡í„°ì˜ ê³± â†’ ê²°ê³¼ëŠ” 2Ã—1\n",
    "#ìµœì¢…ì ìœ¼ë¡œ ğ‘¥âŠ¤ğ´ğ‘¥ë¥¼ ê³„ì‚° â†’ ê²°ê³¼ëŠ” ìŠ¤ì¹¼ë¼ (1Ã—1 í…ì„œ)\n",
    "f = torch.matmul(x.T, torch.matmul(A,x))\n",
    "print(f\"f matrix : {f}\")\n",
    "\n",
    "#backward()ëŠ” í•¨ìˆ˜(function)\n",
    "#fì— ëŒ€í•´ ë¯¸ë¶„ê°’ì„ ìë™ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬, \n",
    "#fì— ì—°ê²°ëœ ëª¨ë“  requires_grad=Trueì¸ í…ì„œë“¤(x ë“±)ì— ëŒ€í•´ \n",
    "#.grad ì†ì„±ì— gradient ì €ì¥\n",
    "# ìˆ˜í•™ì  ì˜ë¯¸ëŠ” fë¥¼ xì— ëŒ€í•´ ë¯¸ë¶„í•œ ê²ƒì´ê³ , ê·¸ ë¯¸ë¶„ê°’ì„ ì €ì¥í•œ ê²ƒì´ë‹¤.\n",
    "f.backward()\n",
    "\n",
    "#item()ì€ í•¨ìˆ˜(function)\n",
    "#1Ã—1 í…ì„œ (ìŠ¤ì¹¼ë¼ í…ì„œ) fì—ì„œ íŒŒì´ì¬ ìˆ«ì ê°’(float)ë¡œ êº¼ë‚´ëŠ” í•¨ìˆ˜\n",
    "print(f\"f = {f.item()}\")\n",
    "\n",
    "#x.gradëŠ” ì†ì„±(attribute)\n",
    "#f.backward() ì‹¤í–‰ ì´í›„ xì— ëŒ€í•´ ê³„ì‚°ëœ ë¯¸ë¶„ ê²°ê³¼(gradient)ë¥¼ ì €ì¥í•œ í…ì„œ\n",
    "print(f\"âˆ‡ f:{x.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe7cec16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f = [[21.]]\n",
      "âˆ‡f = [[10.]\n",
      " [16.]]\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# ë³€ìˆ˜ ì„ ì–¸\n",
    "x = tf.Variable([[1.0], [2.0]])\n",
    "A = tf.constant([[1.0, 2.0], [2.0, 3.0]])\n",
    "\n",
    "# ë¯¸ë¶„\n",
    "with tf.GradientTape() as tape:\n",
    "    f = tf.matmul(tf.transpose(x), tf.matmul(A, x))\n",
    "\n",
    "grad = tape.gradient(f, x)\n",
    "\n",
    "print(\"f =\", f.numpy())\n",
    "print(\"âˆ‡f =\", grad.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b778cea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF generated: gradient_proof_dejavu.pdf\n"
     ]
    }
   ],
   "source": [
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "import os\n",
    "\n",
    "# 1) Locate DejaVuSans.ttf that comes with matplotlib\n",
    "import matplotlib\n",
    "dejavu_path = os.path.join(matplotlib.get_data_path(), \"fonts\", \"ttf\", \"DejaVuSans.ttf\")\n",
    "\n",
    "# 2) Register it under a friendly name\n",
    "pdfmetrics.registerFont(TTFont(\"DejaVuSans\", dejavu_path))\n",
    "\n",
    "# 3) Your proof text (English, with math symbols)\n",
    "proof_text = \"\"\"Mathematical Proof and Intuitive Explanation:\n",
    "Why âˆ‚f/âˆ‚x = 2Ax for f = xáµ—Ax when A is symmetric (A = Aáµ—).\n",
    "\n",
    "Function definition:\n",
    "f(x) = xáµ— A x\n",
    "x âˆˆ â„â¿ : an nÃ—1 column vector\n",
    "A âˆˆ â„â¿Ë£â¿ : a square matrix with A = Aáµ—\n",
    "\n",
    "Goal:\n",
    "Find âˆ‚f/âˆ‚x.\n",
    "\n",
    "[Method 1] Component-wise Expansion:\n",
    "f = xáµ— A x = Î£(i=1 to n) Î£(j=1 to n) xáµ¢ Aáµ¢â±¼ xâ±¼\n",
    "\n",
    "âˆ‚f/âˆ‚xâ‚– = Î£(j=1 to n) Aâ‚–â±¼ xâ±¼ + Î£(i=1 to n) Aáµ¢â‚– xáµ¢\n",
    "       = (Ax)â‚– + (Aáµ—x)â‚–\n",
    "\n",
    "Therefore: âˆ‚f/âˆ‚x = Ax + Aáµ—x\n",
    "\n",
    "[Symmetric Case]\n",
    "If A = Aáµ— then âˆ‚f/âˆ‚x = Ax + Ax = 2Ax.\n",
    "\n",
    "[Method 2] Matrix-Differentiation Formula:\n",
    "For f(x) = xáµ— A x,\n",
    "âˆ‚f/âˆ‚x = \n",
    "  â€¢ 2Ax   if A = Aáµ—\n",
    "  â€¢ Aáµ—x + Ax   in the general case\n",
    "\n",
    "[Intuitive Meaning]\n",
    "f(x) = xáµ— A x is a quadratic form.\n",
    "Geometrically, its gradient points in the 2Ax direction.\n",
    "\n",
    "[Example: 2Ã—2 Case]\n",
    "A = [[1, 2],\n",
    "     [2, 3]]\n",
    "x = [xâ‚, xâ‚‚]áµ—\n",
    "\n",
    "f = xâ‚Â² + 4xâ‚xâ‚‚ + 3xâ‚‚Â²\n",
    "\n",
    "âˆ‚f/âˆ‚xâ‚ = 2xâ‚ + 4xâ‚‚\n",
    "âˆ‚f/âˆ‚xâ‚‚ = 4xâ‚ + 6xâ‚‚\n",
    "\n",
    "Thus âˆ‚f/âˆ‚x = [2xâ‚ + 4xâ‚‚, 4xâ‚ + 6xâ‚‚]áµ— = 2Ax.\n",
    "\"\"\"\n",
    "\n",
    "# 4) Create the PDF\n",
    "pdf_filename = \"gradient_proof_dejavu.pdf\"\n",
    "c = canvas.Canvas(pdf_filename)\n",
    "c.setFont(\"DejaVuSans\", 11)\n",
    "\n",
    "# Simple line-by-line drawing with line-wrap at ~80 chars\n",
    "y = 800\n",
    "for paragraph in proof_text.split(\"\\n\\n\"):\n",
    "    for line in paragraph.splitlines():\n",
    "        c.drawString(40, y, line)\n",
    "        y -= 14\n",
    "        if y < 40:\n",
    "            c.showPage()\n",
    "            c.setFont(\"DejaVuSans\", 11)\n",
    "            y = 800\n",
    "    y -= 10  # extra space between paragraphs\n",
    "\n",
    "c.save()\n",
    "\n",
    "print(\"PDF generated:\", pdf_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2edb953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f = [[21.]]\n",
      "âˆ‡f = [[10.]\n",
      " [16.]]\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# ë³€ìˆ˜ ì„ ì–¸\n",
    "x = tf.Variable([[1.0], [2.0]])\n",
    "A = tf.constant([[1.0, 2.0], [2.0, 3.0]])\n",
    "\n",
    "# ë¯¸ë¶„\n",
    "with tf.GradientTape() as tape:\n",
    "    f = tf.matmul(tf.transpose(x), tf.matmul(A, x))\n",
    "\n",
    "grad = tape.gradient(f, x)\n",
    "\n",
    "print(\"f =\", f.numpy())\n",
    "print(\"âˆ‡f =\", grad.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c009e7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())   # True â†’ GPU ì‚¬ìš© ê°€ëŠ¥ / False â†’ CPU ì „ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e06cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Sequential name=sequential_2, built=False>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisf\\OneDrive\\desktop\\WorkSpace\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              â”‚             \u001b[38;5;34m8\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> (32.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8\u001b[0m (32.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> (32.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8\u001b[0m (32.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "print(model)\n",
    "\n",
    "# 3ê°œì˜ ì…ë ¥ê³¼ 2ê°œì˜ ì¶œë ¥\n",
    "model.add(Dense(2, input_dim =3, activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e71cd793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisf\\OneDrive\\desktop\\WorkSpace\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              â”‚            \u001b[38;5;34m40\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              â”‚            \u001b[38;5;34m72\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              â”‚            \u001b[38;5;34m27\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">139</span> (556.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m139\u001b[0m (556.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">139</span> (556.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m139\u001b[0m (556.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, input_dim = 4, activation ='relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(3, activation ='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e3e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¶œë ¥ ë²¡í„°: [1.1007609447179034, 0, 0, 0.7572516085970521, 0, 0.7196539051623831, 0.5352140492242567, 0.766976580974997]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# ReLU í™œì„±í™” í•¨ìˆ˜\n",
    "def relu(x):\n",
    "    return [max(0, val) for val in x]\n",
    "\n",
    "# Dense Layer í´ë˜ìŠ¤ ì •ì˜\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim, activation=relu):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "\n",
    "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”: WëŠ” input_dim x output_dim í–‰ë ¬\n",
    "        self.weights = [[random.uniform(-1, 1) for _ in range(output_dim)] for _ in range(input_dim)]\n",
    "\n",
    "        # ë°”ì´ì–´ìŠ¤ ì´ˆê¸°í™”: output_dim í¬ê¸°ì˜ ë²¡í„°\n",
    "        self.biases = [random.uniform(-1, 1) for _ in range(output_dim)]\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        if len(input_vector) != self.input_dim:\n",
    "            raise ValueError(\"ì…ë ¥ ë²¡í„° ì°¨ì›ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        # ì„ í˜• ê²°í•©: z = xW + b\n",
    "        z = [0 for _ in range(self.output_dim)]\n",
    "        for j in range(self.output_dim):\n",
    "            for i in range(self.input_dim):\n",
    "                z[j] += input_vector[i] * self.weights[i][j]\n",
    "            z[j] += self.biases[j]\n",
    "\n",
    "        # í™œì„±í™” í•¨ìˆ˜ ì ìš©\n",
    "        return self.activation(z)\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "input_vector = [0.5, -0.2, 0.1, 0.9]\n",
    "dense = DenseLayer(input_dim=4, output_dim=8)\n",
    "output = dense.forward(input_vector)\n",
    "\n",
    "print(\"ì¶œë ¥ ë²¡í„°:\", output)\n",
    "# ì¶œë ¥ ë²¡í„°: [1.1007609447179034, 0, 0, 0.7572516085970521, 0, 0.7196539051623831, 0.5352140492242567, 0.766976580974997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc989694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1: Loss = 5.6215\n",
      "Epoch2: Loss = 4.9021\n",
      "Epoch3: Loss = 4.3225\n",
      "Epoch4: Loss = 3.7863\n",
      "Epoch5: Loss = 3.4286\n",
      "Epoch6: Loss = 3.3146\n",
      "Epoch7: Loss = 3.3861\n",
      "Epoch8: Loss = 3.6765\n",
      "Epoch9: Loss = 3.8822\n",
      "Epoch10: Loss = 2.7682\n",
      "Epoch11: Loss = 1.4083\n",
      "Epoch12: Loss = 0.0187\n",
      "Epoch13: Loss = 0.0138\n",
      "Epoch14: Loss = 0.0118\n",
      "Epoch15: Loss = 0.0107\n",
      "Epoch16: Loss = 0.0100\n",
      "Epoch17: Loss = 0.0095\n",
      "Epoch18: Loss = 0.0091\n",
      "Epoch19: Loss = 0.0088\n",
      "Epoch20: Loss = 0.0085\n",
      "Epoch21: Loss = 0.0082\n",
      "Epoch22: Loss = 0.0080\n",
      "Epoch23: Loss = 0.0077\n",
      "Epoch24: Loss = 0.0075\n",
      "Epoch25: Loss = 0.0073\n",
      "Epoch26: Loss = 0.0071\n",
      "Epoch27: Loss = 0.0069\n",
      "Epoch28: Loss = 0.0067\n",
      "Epoch29: Loss = 0.0066\n",
      "Epoch30: Loss = 0.0064\n",
      "Epoch31: Loss = 0.0063\n",
      "Epoch32: Loss = 0.0061\n",
      "Epoch33: Loss = 0.0060\n",
      "Epoch34: Loss = 0.0058\n",
      "Epoch35: Loss = 0.0057\n",
      "Epoch36: Loss = 0.0056\n",
      "Epoch37: Loss = 0.0055\n",
      "Epoch38: Loss = 0.0053\n",
      "Epoch39: Loss = 0.0052\n",
      "Epoch40: Loss = 0.0051\n",
      "Epoch41: Loss = 0.0050\n",
      "Epoch42: Loss = 0.0049\n",
      "Epoch43: Loss = 0.0048\n",
      "Epoch44: Loss = 0.0047\n",
      "Epoch45: Loss = 0.0047\n",
      "Epoch46: Loss = 0.0046\n",
      "Epoch47: Loss = 0.0045\n",
      "Epoch48: Loss = 0.0044\n",
      "Epoch49: Loss = 0.0043\n",
      "Epoch50: Loss = 0.0043\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# ======== í™œì„±í™” í•¨ìˆ˜ ë° ê·¸ë¼ë””ì–¸íŠ¸ ========\n",
    "# í™œì„±í™” í•¨ìˆ˜ ì •ì˜\n",
    "def relu(x):\n",
    "    return [max(0, val) for val in x]\n",
    "\n",
    "def relu_derivative(x):\n",
    "    # ê¸°ìš¸ê¸°\n",
    "    return [1 if i > 0 else 0 for i in x]\n",
    "\n",
    "def softmax(x):\n",
    "    exps = [math.exp(i) for i in x]\n",
    "    sum_exps = sum(exps)\n",
    "    return [j / sum_exps for j in exps]\n",
    "\n",
    "\n",
    "# ======== ì†ì‹¤ í•¨ìˆ˜ (í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼) ========\n",
    "def cross_entropy(predicted, actual):\n",
    "    return -sum(a * math.log(p + 1e-9) for p, a in zip(predicted, actual))\n",
    "\n",
    "def cross_entropy_derivative(predicted, actual):\n",
    "    return [p - a for p, a in zip(predicted, actual)]\n",
    "\n",
    "\n",
    "# ======== Dense Layer í´ë˜ìŠ¤ ========\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim, activation, activation_derivative):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.weights = [[random.uniform(-1, 1) for _ in range(output_dim)] for _ in range(input_dim)]\n",
    "        self.biases = [random.uniform(-1, 1) for _ in range(output_dim)]\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        self.input = input_vector # for backprop\n",
    "        self.z = [sum(input_vector[i] * self.weights[i][j] for i in range(self.input_dim)) + self.biases[j] for j in range(self.output_dim)]\n",
    "        self.output = self.activation(self.z)\n",
    "        return self.output\n",
    "    \n",
    "    # output_gradient: ë‹¤ìŒ ë ˆì´ì–´(ë˜ëŠ” ì¶œë ¥ì¸µ)ì—ì„œ ì „ë‹¬ëœ gradient (ì¦‰, âˆ‚L/âˆ‚output)\n",
    "    # learning_rate: í•™ìŠµë¥  (gradientì— ê³±í•´ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•  ë•Œ ì‚¬ìš©)\n",
    "    # 1ë‹¨ê³„: dz ê³„ì‚°\n",
    "    # dz = ì—­ì „íŒŒì—ì„œ í•­ìƒ ë‹¤ìŒê³¼ ê°™ì€ chain ruleì„ ì ìš©í•©ë‹ˆë‹¤:\n",
    "    #       âˆ‚L/âˆ‚z = âˆ‚L/âˆ‚a Ã— âˆ‚a/âˆ‚z\n",
    "    #           âˆ‚L/âˆ‚a = output_gradient (ë‹¤ìŒ ë ˆì´ì–´ì—ì„œ ë„˜ì–´ì˜¨ ê°’)\n",
    "    #           âˆ‚a/âˆ‚z = í™œì„±í™” í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜(í˜„ì¬ ë ˆì´ì–´ì— ì •ì˜ëœ activation functionì˜ ë¯¸ë¶„ê°’)\n",
    "    #           ë”°ë¼ì„œ dz[i] = output_gradient[i] Ã— f'(z[i])\n",
    "    # 2ë‹¨ê³„: ê°€ì¤‘ì¹˜ ë° í¸í–¥ gradient ê³„ì‚°\n",
    "    # dw[i][j] = âˆ‚L/âˆ‚W[i][j]\n",
    "    #   ì´ ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ W[i][j]ëŠ” ì…ë ¥ self.input[i]ì™€ ì¶œë ¥ z[j]ì˜ gradient(dz[j])ì— ì˜í•´ ê²°ì •ë©ë‹ˆë‹¤.    \n",
    "    #   ë”°ë¼ì„œ ê° weightì— ëŒ€í•œ gradientëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "    #   âˆ‚L/âˆ‚W[i][j] = input[i] Ã— dz[j]\n",
    "    # ë°”ì´ì–´ìŠ¤(bias)ëŠ” ê°ê° ì¶œë ¥ ë‰´ëŸ°ë§ˆë‹¤ í•˜ë‚˜ì”© ì—°ê²°ë˜ì–´ ìˆìœ¼ë¯€ë¡œ\n",
    "    # ë°”ì´ì–´ìŠ¤ì— ëŒ€í•œ gradientëŠ” ë‹¨ìˆœíˆ dz[j] ê°’ê³¼ ë™ì¼í•©ë‹ˆë‹¤:\n",
    "    #   âˆ‚L/âˆ‚b[j] = dz[j]\n",
    "    # 3ë‹¨ê³„: ê°€ì¤‘ì¹˜ì™€ ë°”ì´ì–´ìŠ¤ ì—…ë°ì´íŠ¸\n",
    "    # SGD(í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•)ë¥¼ ì‚¬ìš©í•œ weight ê°±ì‹ \n",
    "    # ê¸°ì¡´ ê°€ì¤‘ì¹˜ì—ì„œ gradientì˜ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ì´ë™:\n",
    "    #   W[i][j] â† W[i][j] - Î· Ã— âˆ‚L/âˆ‚W[i][j]\n",
    "    # í¸í–¥ë„ ìœ„ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸:\n",
    "    #   b[j] â† b[j] - Î· Ã— âˆ‚L/âˆ‚b[j]\n",
    "    # 4ë‹¨ê³„: ì´ì „ ë ˆì´ì–´ë¡œ ë³´ë‚¼ gradient ê³„ì‚°\n",
    "    # ë‹¤ìŒ ë ˆì´ì–´ì˜ ì…ë ¥ì´ì, í˜„ì¬ ë ˆì´ì–´ì˜ ì…ë ¥ x[i]ì— ëŒ€í•œ gradientë¥¼ ê³„ì‚°\n",
    "    # ì¦‰, ì´ ë ˆì´ì–´ì˜ ì…ë ¥ x[i]ê°€ ì „ì²´ ì†ì‹¤ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠ”ì§€ ê³„ì‚°\n",
    "    #   âˆ‚L/âˆ‚x[i] = âˆ‘(W[i][j] Ã— dz[j]) over j\n",
    "    #   ì´ê²ƒì„ ì´ì „ ë ˆì´ì–´ì˜ backward í•¨ìˆ˜ë¡œ ì „ë‹¬\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        dz = [output_gradient[i] * self.activation_derivative([self.z[i]])[0] for i in range(self.output_dim)]\n",
    "\n",
    "        # weight, bias gradients\n",
    "        dw = [[self.input[i] * dz[j] for j in range(self.output_dim)] for i in range(self.input_dim)]\n",
    "        db = dz[:]\n",
    "\n",
    "        #update weights and biases\n",
    "        for i in range(self.input_dim):\n",
    "            for j in range(self.output_dim):\n",
    "                self.weights[i][j] -= learning_rate * dw[i][j]\n",
    "\n",
    "        for j in range(self.output_dim):\n",
    "            self.biases[j] -= learning_rate * db[j]\n",
    "\n",
    "        # return gradient for previous layer\n",
    "        prev_grad = [sum(self.weights[i][j] * dz[j] for j in range(self.output_dim)) for i in range(self.input_dim)]\n",
    "        return prev_grad\n",
    "\n",
    "# ======== Softmax + Cross Entropy Output Layer ========\n",
    "class SoftmaxLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = [[random.uniform(-0.5, 0.5) for _ in range(output_dim)] for _ in range(input_dim)]\n",
    "        self.biases = [0.0 for _ in range(output_dim)]\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        self.input = input_vector\n",
    "        self.z = [sum(input_vector[i] * self.weights[i][j] for i in range(self.input_dim)) + self.biases[j] for j in range(self.output_dim)] \n",
    "        self.output = softmax(self.z)\n",
    "        return self.output\n",
    "    \n",
    "    # ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´(dz)ë¥¼ êµ¬í•œ ë‹¤ìŒì—\n",
    "    # í˜„ì¬ ë ˆì´ì–´ë¡œ ë“¤ì–´ì˜¨ ì…ë ¥ê°’ê³¼ ê·¸ ì°¨ì´(dz)ë¥¼ ê³±í•´ì„œ dw ë²¡í„°ë¥¼ ë§Œë“¤ê³ ,\n",
    "    # db ë²¡í„°ë„ dzì—ì„œ ê°€ì ¸ì™€ì„œ ë§Œë“¤ê³ ,\n",
    "    # í˜„ì¬ ê°€ì¤‘ì¹˜ì—ì„œ (dw Ã— í•™ìŠµë¥ )ì„ ë¹¼ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³ ,\n",
    "    # í¸í–¥ë„ ë™ì¼í•˜ê²Œ ì—…ë°ì´íŠ¸í•œ í›„,\n",
    "    # ì´ ìƒˆë¡œ ê³„ì‚°í•œ  ê°€ì¤‘ì¹˜ì™€ dzì™€ ê³±í•œ í›„ ë”í•œ gradientë¥¼ ì´ì „ ë ˆì´ì–´ë¡œ ë„˜ê²¨ ë‹¤ìŒ ë ˆì´ì–´ë„ ê°™ì€ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤.\n",
    "    def backward(self, target_vector, learning_rate):\n",
    "        # derivative of cross entropy + softamx\n",
    "        # dz = y_hat - y_real\n",
    "        # dz = [-0.3, 0.2, 0.1] if self.output(pred) = [0.7, 0.2, 0.1] and target_vector(real) = [1, 0, 0]\n",
    "        dz = cross_entropy_derivative(self.output, target_vector) \n",
    "\n",
    "        # weight, bias gradients\n",
    "        # dw[i,j] : weights[i][j] ì— ëŒ€í•œ í¸ë¯¸ë¶„\n",
    "        # self.input[i] : ì´ ë ˆì´ì–´ë¡œ ë“¤ì–´ì˜¨ ì…ë ¥ ê°’(ì• ë ˆì´ì–´ì˜ ì¶œë ¥)\n",
    "        # dz[j] : ì†ì‹¤ì— ëŒ€í•œ jë²ˆì§¸ ì¶œë ¥ ë‰´ëŸ°ì˜ gradient\n",
    "        # ì¦‰, âˆ‚L/âˆ‚W[i][j] = input[i] * dz[j]\n",
    "        # ê°€ì¤‘ì¹˜ Wì˜ ê° í•­ëª©ì€ ì…ë ¥ê°’ì— ê³±í•´ì§€ë¯€ë¡œ, Wì˜ gradientëŠ” ì…ë ¥ê°’ Ã— dzì…ë‹ˆë‹¤.\n",
    "        dw = [[self.input[i] * dz[j] for j in range(self.output_dim)] for i in range(self.input_dim)]\n",
    "        \n",
    "        # í¸í–¥(biases)ì˜ gradientëŠ” dzì™€ ë™ì¼í•˜ë‹¤.\n",
    "        # âˆ‚L/âˆ‚b[j] = dz[j]ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë³µì‚¬\n",
    "        db = dz[:]\n",
    "\n",
    "        # update\n",
    "        # SGD ë°©ì‹ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸\n",
    "        # ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ = ê¸°ì¡´ ê°€ì¤‘ì¹˜ - í•™ìŠµë¥  Ã— gradient\n",
    "        # ì†ì‹¤ì„ ì¤„ì´ê¸° ìœ„í•´ gradient ë°©í–¥ì˜ ë°˜ëŒ€ë¡œ ì´ë™\n",
    "        for i in range(self.input_dim):\n",
    "            for j in range(self.output_dim):\n",
    "                self.weights[i][j] -= learning_rate * dw[i][j]\n",
    "\n",
    "        # ìœ„ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ bias ì—…ë°ì´íŠ¸\n",
    "        for j in range(self.output_dim):\n",
    "            self.biases[j] -= learning_rate * db[j]\n",
    "\n",
    "        # return gradient for previous layer\n",
    "        # ë‹¤ìŒ ì—­ì „íŒŒë¥¼ ìœ„í•´ ì´ì „ ë ˆì´ì–´ë¡œ ë„˜ê¸¸ gradientë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "        # âˆ‚L/âˆ‚x_i = âˆ‘(W[i][j] * dz[j]) í˜•íƒœ\n",
    "        # í˜„ì¬ ë ˆì´ì–´ì˜ ì¶œë ¥ì€ ì´ì „ ë ˆì´ì–´ì˜ ì…ë ¥ì— ì˜ì¡´í•˜ë¯€ë¡œ, chain ruleì„ ì ìš©\n",
    "        prev_grad = [sum(self.weights[i][j] * dz[j] for j in range(self.output_dim)) for i in range(self.input_dim)]\n",
    "        return prev_grad\n",
    "\n",
    "\n",
    "\n",
    "# Sequential ëª¨ë¸ ì •ì˜\n",
    "class SequentialModel:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, y_true, learning_rate):\n",
    "        grad = y_true\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad, learning_rate)\n",
    "    \n",
    "    def train(self, x, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for xi, yi in zip(x, y):\n",
    "                y_pred = self.forward(xi)\n",
    "                total_loss += cross_entropy(y_pred, yi)\n",
    "                self.backward(yi, learning_rate)\n",
    "            \n",
    "            print(f\"Epoch{epoch + 1}: Loss = {total_loss:.4f}\")\n",
    "\n",
    "    def summary(self):\n",
    "        print(\"ëª¨ë¸ ìš”ì•½:\")\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            print(f\" Layer {idx + 1}: ì…ë ¥ {layer.input_dim}, ì¶œë ¥ {layer.output_dim}, í™œì„±í™” {layer.activation.__name__}\")\n",
    "\n",
    "\n",
    "# ======== í•™ìŠµ ë°ì´í„° (ê°„ë‹¨í•œ ë¶„ë¥˜ ë¬¸ì œ) ========\n",
    "# ì…ë ¥: 4ì°¨ì› / ì¶œë ¥: one-hot 3ì°¨ì›\n",
    "train_x = [\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "]\n",
    "\n",
    "train_y = [\n",
    "    [1, 0, 0],  # class 0\n",
    "    [0, 1, 0],  # class 1\n",
    "    [0, 0, 1],  # class 2\n",
    "    [1, 0, 0],  # class 0\n",
    "]\n",
    "\n",
    "# ======== ëª¨ë¸ êµ¬ì„± ë° í•™ìŠµ ========\n",
    "model = SequentialModel()\n",
    "model.add(DenseLayer(input_dim=4, output_dim=8, activation=relu, activation_derivative=relu_derivative))\n",
    "model.add(DenseLayer(input_dim=8, output_dim=8, activation=relu, activation_derivative=relu_derivative))\n",
    "model.add(SoftmaxLayer(input_dim=8, output_dim=3))\n",
    "\n",
    "model.train(train_x, train_y, epochs=50, learning_rate=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
