{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "487d02f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f matrix : tensor([[21.]], grad_fn=<MmBackward0>)\n",
      "f = 21.0\n",
      "∇ f:tensor([[10.],\n",
      "        [16.]])\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch\n",
    "\n",
    "import torch\n",
    "\n",
    "#torch.tensor()는 함수(function)\n",
    "#[[1.0], [2.0]]: 2×1 열 벡터 (Column vector)\n",
    "#requires_grad=True: 이후 미분(gradient)을 계산할 수 있도록 자동 미분 트래킹 활성화\n",
    "x = torch.tensor([[1.0], [2.0]], requires_grad=True)\n",
    "\n",
    "#2×2 실수 행렬을 생성. \n",
    "#기본적으로 requires_grad=False이므로 A에 대해 미분은 계산되지 않음\n",
    "A = torch.tensor([[1.0, 2.0], [2.0, 3.0]])\n",
    "\n",
    "#torch.matmul()은 함수(function)\n",
    "#torch.matmul(A, x) → Ax: 2×2 행렬과 2×1 벡터의 곱 → 결과는 2×1\n",
    "#최종적으로 𝑥⊤𝐴𝑥를 계산 → 결과는 스칼라 (1×1 텐서)\n",
    "f = torch.matmul(x.T, torch.matmul(A,x))\n",
    "print(f\"f matrix : {f}\")\n",
    "\n",
    "#backward()는 함수(function)\n",
    "#f에 대해 미분값을 자동으로 계산하여, \n",
    "#f에 연결된 모든 requires_grad=True인 텐서들(x 등)에 대해 \n",
    "#.grad 속성에 gradient 저장\n",
    "# 수학적 의미는 f를 x에 대해 미분한 것이고, 그 미분값을 저장한 것이다.\n",
    "f.backward()\n",
    "\n",
    "#item()은 함수(function)\n",
    "#1×1 텐서 (스칼라 텐서) f에서 파이썬 숫자 값(float)로 꺼내는 함수\n",
    "print(f\"f = {f.item()}\")\n",
    "\n",
    "#x.grad는 속성(attribute)\n",
    "#f.backward() 실행 이후 x에 대해 계산된 미분 결과(gradient)를 저장한 텐서\n",
    "print(f\"∇ f:{x.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe7cec16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f = [[21.]]\n",
      "∇f = [[10.]\n",
      " [16.]]\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 변수 선언\n",
    "x = tf.Variable([[1.0], [2.0]])\n",
    "A = tf.constant([[1.0, 2.0], [2.0, 3.0]])\n",
    "\n",
    "# 미분\n",
    "with tf.GradientTape() as tape:\n",
    "    f = tf.matmul(tf.transpose(x), tf.matmul(A, x))\n",
    "\n",
    "grad = tape.gradient(f, x)\n",
    "\n",
    "print(\"f =\", f.numpy())\n",
    "print(\"∇f =\", grad.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b778cea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF generated: gradient_proof_dejavu.pdf\n"
     ]
    }
   ],
   "source": [
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "import os\n",
    "\n",
    "# 1) Locate DejaVuSans.ttf that comes with matplotlib\n",
    "import matplotlib\n",
    "dejavu_path = os.path.join(matplotlib.get_data_path(), \"fonts\", \"ttf\", \"DejaVuSans.ttf\")\n",
    "\n",
    "# 2) Register it under a friendly name\n",
    "pdfmetrics.registerFont(TTFont(\"DejaVuSans\", dejavu_path))\n",
    "\n",
    "# 3) Your proof text (English, with math symbols)\n",
    "proof_text = \"\"\"Mathematical Proof and Intuitive Explanation:\n",
    "Why ∂f/∂x = 2Ax for f = xᵗAx when A is symmetric (A = Aᵗ).\n",
    "\n",
    "Function definition:\n",
    "f(x) = xᵗ A x\n",
    "x ∈ ℝⁿ : an n×1 column vector\n",
    "A ∈ ℝⁿˣⁿ : a square matrix with A = Aᵗ\n",
    "\n",
    "Goal:\n",
    "Find ∂f/∂x.\n",
    "\n",
    "[Method 1] Component-wise Expansion:\n",
    "f = xᵗ A x = Σ(i=1 to n) Σ(j=1 to n) xᵢ Aᵢⱼ xⱼ\n",
    "\n",
    "∂f/∂xₖ = Σ(j=1 to n) Aₖⱼ xⱼ + Σ(i=1 to n) Aᵢₖ xᵢ\n",
    "       = (Ax)ₖ + (Aᵗx)ₖ\n",
    "\n",
    "Therefore: ∂f/∂x = Ax + Aᵗx\n",
    "\n",
    "[Symmetric Case]\n",
    "If A = Aᵗ then ∂f/∂x = Ax + Ax = 2Ax.\n",
    "\n",
    "[Method 2] Matrix-Differentiation Formula:\n",
    "For f(x) = xᵗ A x,\n",
    "∂f/∂x = \n",
    "  • 2Ax   if A = Aᵗ\n",
    "  • Aᵗx + Ax   in the general case\n",
    "\n",
    "[Intuitive Meaning]\n",
    "f(x) = xᵗ A x is a quadratic form.\n",
    "Geometrically, its gradient points in the 2Ax direction.\n",
    "\n",
    "[Example: 2×2 Case]\n",
    "A = [[1, 2],\n",
    "     [2, 3]]\n",
    "x = [x₁, x₂]ᵗ\n",
    "\n",
    "f = x₁² + 4x₁x₂ + 3x₂²\n",
    "\n",
    "∂f/∂x₁ = 2x₁ + 4x₂\n",
    "∂f/∂x₂ = 4x₁ + 6x₂\n",
    "\n",
    "Thus ∂f/∂x = [2x₁ + 4x₂, 4x₁ + 6x₂]ᵗ = 2Ax.\n",
    "\"\"\"\n",
    "\n",
    "# 4) Create the PDF\n",
    "pdf_filename = \"gradient_proof_dejavu.pdf\"\n",
    "c = canvas.Canvas(pdf_filename)\n",
    "c.setFont(\"DejaVuSans\", 11)\n",
    "\n",
    "# Simple line-by-line drawing with line-wrap at ~80 chars\n",
    "y = 800\n",
    "for paragraph in proof_text.split(\"\\n\\n\"):\n",
    "    for line in paragraph.splitlines():\n",
    "        c.drawString(40, y, line)\n",
    "        y -= 14\n",
    "        if y < 40:\n",
    "            c.showPage()\n",
    "            c.setFont(\"DejaVuSans\", 11)\n",
    "            y = 800\n",
    "    y -= 10  # extra space between paragraphs\n",
    "\n",
    "c.save()\n",
    "\n",
    "print(\"PDF generated:\", pdf_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2edb953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f = [[21.]]\n",
      "∇f = [[10.]\n",
      " [16.]]\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 변수 선언\n",
    "x = tf.Variable([[1.0], [2.0]])\n",
    "A = tf.constant([[1.0, 2.0], [2.0, 3.0]])\n",
    "\n",
    "# 미분\n",
    "with tf.GradientTape() as tape:\n",
    "    f = tf.matmul(tf.transpose(x), tf.matmul(A, x))\n",
    "\n",
    "grad = tape.gradient(f, x)\n",
    "\n",
    "print(\"f =\", f.numpy())\n",
    "print(\"∇f =\", grad.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c009e7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())   # True → GPU 사용 가능 / False → CPU 전용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e06cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Sequential name=sequential_2, built=False>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisf\\OneDrive\\desktop\\WorkSpace\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m8\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> (32.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8\u001b[0m (32.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> (32.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8\u001b[0m (32.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "print(model)\n",
    "\n",
    "# 3개의 입력과 2개의 출력\n",
    "model.add(Dense(2, input_dim =3, activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e71cd793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisf\\OneDrive\\desktop\\WorkSpace\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m40\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m72\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m27\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">139</span> (556.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m139\u001b[0m (556.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">139</span> (556.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m139\u001b[0m (556.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, input_dim = 4, activation ='relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(3, activation ='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e3e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "출력 벡터: [1.1007609447179034, 0, 0, 0.7572516085970521, 0, 0.7196539051623831, 0.5352140492242567, 0.766976580974997]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# ReLU 활성화 함수\n",
    "def relu(x):\n",
    "    return [max(0, val) for val in x]\n",
    "\n",
    "# Dense Layer 클래스 정의\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim, activation=relu):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "\n",
    "        # 가중치 초기화: W는 input_dim x output_dim 행렬\n",
    "        self.weights = [[random.uniform(-1, 1) for _ in range(output_dim)] for _ in range(input_dim)]\n",
    "\n",
    "        # 바이어스 초기화: output_dim 크기의 벡터\n",
    "        self.biases = [random.uniform(-1, 1) for _ in range(output_dim)]\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        if len(input_vector) != self.input_dim:\n",
    "            raise ValueError(\"입력 벡터 차원이 일치하지 않습니다.\")\n",
    "\n",
    "        # 선형 결합: z = xW + b\n",
    "        z = [0 for _ in range(self.output_dim)]\n",
    "        for j in range(self.output_dim):\n",
    "            for i in range(self.input_dim):\n",
    "                z[j] += input_vector[i] * self.weights[i][j]\n",
    "            z[j] += self.biases[j]\n",
    "\n",
    "        # 활성화 함수 적용\n",
    "        return self.activation(z)\n",
    "\n",
    "# 사용 예시\n",
    "input_vector = [0.5, -0.2, 0.1, 0.9]\n",
    "dense = DenseLayer(input_dim=4, output_dim=8)\n",
    "output = dense.forward(input_vector)\n",
    "\n",
    "print(\"출력 벡터:\", output)\n",
    "# 출력 벡터: [1.1007609447179034, 0, 0, 0.7572516085970521, 0, 0.7196539051623831, 0.5352140492242567, 0.766976580974997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc989694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1: Loss = 5.6215\n",
      "Epoch2: Loss = 4.9021\n",
      "Epoch3: Loss = 4.3225\n",
      "Epoch4: Loss = 3.7863\n",
      "Epoch5: Loss = 3.4286\n",
      "Epoch6: Loss = 3.3146\n",
      "Epoch7: Loss = 3.3861\n",
      "Epoch8: Loss = 3.6765\n",
      "Epoch9: Loss = 3.8822\n",
      "Epoch10: Loss = 2.7682\n",
      "Epoch11: Loss = 1.4083\n",
      "Epoch12: Loss = 0.0187\n",
      "Epoch13: Loss = 0.0138\n",
      "Epoch14: Loss = 0.0118\n",
      "Epoch15: Loss = 0.0107\n",
      "Epoch16: Loss = 0.0100\n",
      "Epoch17: Loss = 0.0095\n",
      "Epoch18: Loss = 0.0091\n",
      "Epoch19: Loss = 0.0088\n",
      "Epoch20: Loss = 0.0085\n",
      "Epoch21: Loss = 0.0082\n",
      "Epoch22: Loss = 0.0080\n",
      "Epoch23: Loss = 0.0077\n",
      "Epoch24: Loss = 0.0075\n",
      "Epoch25: Loss = 0.0073\n",
      "Epoch26: Loss = 0.0071\n",
      "Epoch27: Loss = 0.0069\n",
      "Epoch28: Loss = 0.0067\n",
      "Epoch29: Loss = 0.0066\n",
      "Epoch30: Loss = 0.0064\n",
      "Epoch31: Loss = 0.0063\n",
      "Epoch32: Loss = 0.0061\n",
      "Epoch33: Loss = 0.0060\n",
      "Epoch34: Loss = 0.0058\n",
      "Epoch35: Loss = 0.0057\n",
      "Epoch36: Loss = 0.0056\n",
      "Epoch37: Loss = 0.0055\n",
      "Epoch38: Loss = 0.0053\n",
      "Epoch39: Loss = 0.0052\n",
      "Epoch40: Loss = 0.0051\n",
      "Epoch41: Loss = 0.0050\n",
      "Epoch42: Loss = 0.0049\n",
      "Epoch43: Loss = 0.0048\n",
      "Epoch44: Loss = 0.0047\n",
      "Epoch45: Loss = 0.0047\n",
      "Epoch46: Loss = 0.0046\n",
      "Epoch47: Loss = 0.0045\n",
      "Epoch48: Loss = 0.0044\n",
      "Epoch49: Loss = 0.0043\n",
      "Epoch50: Loss = 0.0043\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# ======== 활성화 함수 및 그라디언트 ========\n",
    "# 활성화 함수 정의\n",
    "def relu(x):\n",
    "    return [max(0, val) for val in x]\n",
    "\n",
    "def relu_derivative(x):\n",
    "    # 기울기\n",
    "    return [1 if i > 0 else 0 for i in x]\n",
    "\n",
    "def softmax(x):\n",
    "    exps = [math.exp(i) for i in x]\n",
    "    sum_exps = sum(exps)\n",
    "    return [j / sum_exps for j in exps]\n",
    "\n",
    "\n",
    "# ======== 손실 함수 (크로스 엔트로피) ========\n",
    "def cross_entropy(predicted, actual):\n",
    "    return -sum(a * math.log(p + 1e-9) for p, a in zip(predicted, actual))\n",
    "\n",
    "def cross_entropy_derivative(predicted, actual):\n",
    "    return [p - a for p, a in zip(predicted, actual)]\n",
    "\n",
    "\n",
    "# ======== Dense Layer 클래스 ========\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim, activation, activation_derivative):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.weights = [[random.uniform(-1, 1) for _ in range(output_dim)] for _ in range(input_dim)]\n",
    "        self.biases = [random.uniform(-1, 1) for _ in range(output_dim)]\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        self.input = input_vector # for backprop\n",
    "        self.z = [sum(input_vector[i] * self.weights[i][j] for i in range(self.input_dim)) + self.biases[j] for j in range(self.output_dim)]\n",
    "        self.output = self.activation(self.z)\n",
    "        return self.output\n",
    "    \n",
    "    # output_gradient: 다음 레이어(또는 출력층)에서 전달된 gradient (즉, ∂L/∂output)\n",
    "    # learning_rate: 학습률 (gradient에 곱해서 가중치를 업데이트할 때 사용)\n",
    "    # 1단계: dz 계산\n",
    "    # dz = 역전파에서 항상 다음과 같은 chain rule을 적용합니다:\n",
    "    #       ∂L/∂z = ∂L/∂a × ∂a/∂z\n",
    "    #           ∂L/∂a = output_gradient (다음 레이어에서 넘어온 값)\n",
    "    #           ∂a/∂z = 활성화 함수의 도함수(현재 레이어에 정의된 activation function의 미분값)\n",
    "    #           따라서 dz[i] = output_gradient[i] × f'(z[i])\n",
    "    # 2단계: 가중치 및 편향 gradient 계산\n",
    "    # dw[i][j] = ∂L/∂W[i][j]\n",
    "    #   이 레이어의 가중치 W[i][j]는 입력 self.input[i]와 출력 z[j]의 gradient(dz[j])에 의해 결정됩니다.    \n",
    "    #   따라서 각 weight에 대한 gradient는 다음과 같습니다:\n",
    "    #   ∂L/∂W[i][j] = input[i] × dz[j]\n",
    "    # 바이어스(bias)는 각각 출력 뉴런마다 하나씩 연결되어 있으므로\n",
    "    # 바이어스에 대한 gradient는 단순히 dz[j] 값과 동일합니다:\n",
    "    #   ∂L/∂b[j] = dz[j]\n",
    "    # 3단계: 가중치와 바이어스 업데이트\n",
    "    # SGD(확률적 경사하강법)를 사용한 weight 갱신\n",
    "    # 기존 가중치에서 gradient의 반대방향으로 이동:\n",
    "    #   W[i][j] ← W[i][j] - η × ∂L/∂W[i][j]\n",
    "    # 편향도 위와 동일한 방식으로 업데이트:\n",
    "    #   b[j] ← b[j] - η × ∂L/∂b[j]\n",
    "    # 4단계: 이전 레이어로 보낼 gradient 계산\n",
    "    # 다음 레이어의 입력이자, 현재 레이어의 입력 x[i]에 대한 gradient를 계산\n",
    "    # 즉, 이 레이어의 입력 x[i]가 전체 손실에 어떤 영향을 주는지 계산\n",
    "    #   ∂L/∂x[i] = ∑(W[i][j] × dz[j]) over j\n",
    "    #   이것을 이전 레이어의 backward 함수로 전달\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        dz = [output_gradient[i] * self.activation_derivative([self.z[i]])[0] for i in range(self.output_dim)]\n",
    "\n",
    "        # weight, bias gradients\n",
    "        dw = [[self.input[i] * dz[j] for j in range(self.output_dim)] for i in range(self.input_dim)]\n",
    "        db = dz[:]\n",
    "\n",
    "        #update weights and biases\n",
    "        for i in range(self.input_dim):\n",
    "            for j in range(self.output_dim):\n",
    "                self.weights[i][j] -= learning_rate * dw[i][j]\n",
    "\n",
    "        for j in range(self.output_dim):\n",
    "            self.biases[j] -= learning_rate * db[j]\n",
    "\n",
    "        # return gradient for previous layer\n",
    "        prev_grad = [sum(self.weights[i][j] * dz[j] for j in range(self.output_dim)) for i in range(self.input_dim)]\n",
    "        return prev_grad\n",
    "\n",
    "# ======== Softmax + Cross Entropy Output Layer ========\n",
    "class SoftmaxLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = [[random.uniform(-0.5, 0.5) for _ in range(output_dim)] for _ in range(input_dim)]\n",
    "        self.biases = [0.0 for _ in range(output_dim)]\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        self.input = input_vector\n",
    "        self.z = [sum(input_vector[i] * self.weights[i][j] for i in range(self.input_dim)) + self.biases[j] for j in range(self.output_dim)] \n",
    "        self.output = softmax(self.z)\n",
    "        return self.output\n",
    "    \n",
    "    # 실제값과 예측값의 차이(dz)를 구한 다음에\n",
    "    # 현재 레이어로 들어온 입력값과 그 차이(dz)를 곱해서 dw 벡터를 만들고,\n",
    "    # db 벡터도 dz에서 가져와서 만들고,\n",
    "    # 현재 가중치에서 (dw × 학습률)을 빼서 가중치를 업데이트하고,\n",
    "    # 편향도 동일하게 업데이트한 후,\n",
    "    # 이 새로 계산한  가중치와 dz와 곱한 후 더한 gradient를 이전 레이어로 넘겨 다음 레이어도 같은 과정을 반복한다.\n",
    "    def backward(self, target_vector, learning_rate):\n",
    "        # derivative of cross entropy + softamx\n",
    "        # dz = y_hat - y_real\n",
    "        # dz = [-0.3, 0.2, 0.1] if self.output(pred) = [0.7, 0.2, 0.1] and target_vector(real) = [1, 0, 0]\n",
    "        dz = cross_entropy_derivative(self.output, target_vector) \n",
    "\n",
    "        # weight, bias gradients\n",
    "        # dw[i,j] : weights[i][j] 에 대한 편미분\n",
    "        # self.input[i] : 이 레이어로 들어온 입력 값(앞 레이어의 출력)\n",
    "        # dz[j] : 손실에 대한 j번째 출력 뉴런의 gradient\n",
    "        # 즉, ∂L/∂W[i][j] = input[i] * dz[j]\n",
    "        # 가중치 W의 각 항목은 입력값에 곱해지므로, W의 gradient는 입력값 × dz입니다.\n",
    "        dw = [[self.input[i] * dz[j] for j in range(self.output_dim)] for i in range(self.input_dim)]\n",
    "        \n",
    "        # 편향(biases)의 gradient는 dz와 동일하다.\n",
    "        # ∂L/∂b[j] = dz[j]이므로 그대로 복사\n",
    "        db = dz[:]\n",
    "\n",
    "        # update\n",
    "        # SGD 방식으로 가중치를 업데이트\n",
    "        # 새로운 가중치 = 기존 가중치 - 학습률 × gradient\n",
    "        # 손실을 줄이기 위해 gradient 방향의 반대로 이동\n",
    "        for i in range(self.input_dim):\n",
    "            for j in range(self.output_dim):\n",
    "                self.weights[i][j] -= learning_rate * dw[i][j]\n",
    "\n",
    "        # 위와 동일한 방식으로 bias 업데이트\n",
    "        for j in range(self.output_dim):\n",
    "            self.biases[j] -= learning_rate * db[j]\n",
    "\n",
    "        # return gradient for previous layer\n",
    "        # 다음 역전파를 위해 이전 레이어로 넘길 gradient를 계산합니다.\n",
    "        # ∂L/∂x_i = ∑(W[i][j] * dz[j]) 형태\n",
    "        # 현재 레이어의 출력은 이전 레이어의 입력에 의존하므로, chain rule을 적용\n",
    "        prev_grad = [sum(self.weights[i][j] * dz[j] for j in range(self.output_dim)) for i in range(self.input_dim)]\n",
    "        return prev_grad\n",
    "\n",
    "\n",
    "\n",
    "# Sequential 모델 정의\n",
    "class SequentialModel:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, y_true, learning_rate):\n",
    "        grad = y_true\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad, learning_rate)\n",
    "    \n",
    "    def train(self, x, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for xi, yi in zip(x, y):\n",
    "                y_pred = self.forward(xi)\n",
    "                total_loss += cross_entropy(y_pred, yi)\n",
    "                self.backward(yi, learning_rate)\n",
    "            \n",
    "            print(f\"Epoch{epoch + 1}: Loss = {total_loss:.4f}\")\n",
    "\n",
    "    def summary(self):\n",
    "        print(\"모델 요약:\")\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            print(f\" Layer {idx + 1}: 입력 {layer.input_dim}, 출력 {layer.output_dim}, 활성화 {layer.activation.__name__}\")\n",
    "\n",
    "\n",
    "# ======== 학습 데이터 (간단한 분류 문제) ========\n",
    "# 입력: 4차원 / 출력: one-hot 3차원\n",
    "train_x = [\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "]\n",
    "\n",
    "train_y = [\n",
    "    [1, 0, 0],  # class 0\n",
    "    [0, 1, 0],  # class 1\n",
    "    [0, 0, 1],  # class 2\n",
    "    [1, 0, 0],  # class 0\n",
    "]\n",
    "\n",
    "# ======== 모델 구성 및 학습 ========\n",
    "model = SequentialModel()\n",
    "model.add(DenseLayer(input_dim=4, output_dim=8, activation=relu, activation_derivative=relu_derivative))\n",
    "model.add(DenseLayer(input_dim=8, output_dim=8, activation=relu, activation_derivative=relu_derivative))\n",
    "model.add(SoftmaxLayer(input_dim=8, output_dim=3))\n",
    "\n",
    "model.train(train_x, train_y, epochs=50, learning_rate=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
